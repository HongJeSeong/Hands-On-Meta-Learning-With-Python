{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Using Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will understand the siamese network by building the face recognition model. The objective of our network is to understand whether two faces are similar or dissimilar. We use AT & T's the Database of Faces which can be downloaded from here (https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html)\n",
    "\n",
    "Once you have downloaded and extracted the archive, you can see the folders like s1, s2 up to s40 as shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these folders has 10 different images of a single person taken from various angles. For an instance, let us open folder s1. As you can see, there are 10 different images of a single person:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 폴더는 다양한 각도에서 찍은 한 사람의 10개의 다른 이미지를 가지고 있다. 예를 들어, s1 폴더를 열자. 보다시피 한 사람의 이미지는 다음과 같이 10가지다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will open and check folder s13,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that siamese networks require inputs as a pair along with the label, we have to create our data in such a way. So we will take two images randomly from the same folder and mark it as a genuine pair and we will take a single image from two different folders and mark them as an imposite pair. A sample data is shown in the below figure, as you can notice a genuine pair has images of the same person and imposite pair has images of a different person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샴 네트워크는 라벨과 함께 쌍으로 입력을 필요로 한다는 것을 우리가 알고 있듯이, 우리는 그러한 방식으로 데이터를 생성해야 한다. 그래서 우리는 같은 폴더에서 무작위로 두 개의 이미지를 찍어서 genuine 쌍으로 표시하고, 다른 두 폴더에서 하나의 이미지를 찍어서 imposite 쌍으로 표시한다. 아래 그림에는 샘플 데이터가 나와 있는데, 실제 한 쌍은쌍은 다른 사람의 이미지를 가지고 있다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data as pairs along with their labels, we train our siamese network. From the image pair, we feed one image to the network A and another image to the network B. The role of these two networks is only to extract the feature vectors. So, we use two convolution layers with relu activations for extracting the features. Once we have learned the feature, we feed the resultant feature vector from both of the networks to the energy function which measures the similarity,  we use Euclidean distance as our energy function. So, we train our network by feeding the image pair to learn the semantic similarity between them.  Now, we will see this step by step. \n",
    "\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 우리가 그들의 라벨과 함께 우리의 데이터를 쌍으로 얻으면, 우리는 샴 네트워크를 훈련시킨다. 이미지 쌍에서 네트워크 A에 이미지 하나를, 네트워크 B에 이미지 하나를 공급한다. 이 두 네트워크의 역할은 오직 피처 벡터를 추출하는 것이다. 그래서 우리는 특징들을 추출하기 위해 두 개의 콘볼루션 레이어와 함께 relu 활성화 함수를 사용한다. 피처를 알게 되면, 우리는 두 네트워크로부터 유사성을 측정하는 에너지 함수에 결과적인 형상 벡터를 공급하고, 우리의 에너지 함수로 유클리드 거리를 사용한다. 그래서, 우리는 이미지 쌍을 공급하여 그들 사이의 의미적 유사성을 학습함으로써 우리의 네트워크를 훈련시킨다. 이제 우리는 이것을 차근차근 보게 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we define a function for reading our input image. The function read_image takes input as an image and returns the numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_image 함수는 이미지를 입력으로 하며 numpy 배열을 반환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filename, byteorder='>'):\n",
    "    \n",
    "    #first we read the image, as a raw file to the buffer\n",
    "    with open(filename, 'rb') as f:\n",
    "        buffer = f.read()\n",
    "    \n",
    "    #using regex, we extract the header, width, height and maxval of the image\n",
    "    header, width, height, maxval = re.search(\n",
    "        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
    "    \n",
    "    #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\n",
    "    return np.frombuffer(buffer,\n",
    "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
    "                            count=int(width)*int(height),\n",
    "                            offset=len(header)\n",
    "                            ).reshape((int(height), int(width)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example, Let us open one image,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PpmImagePlugin.PpmImageFile image mode=L size=92x112 at 0x7FBBE0891240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(\"data/orl_faces/s1/1.pgm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we feed this image to our read_image function, it will return as the numpy array,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 이미지를 read_image() 함수에 넣었을 때 numpy 배열을 반환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 49 45 47 49 57 39 42 53 49 53 60 76 91 99 95 80 75 66 54 47 49 50 43 46 53 61 70 84 105 133 130 110 94 81 107 95 80 57 55 66 86 80 74 65 71 62 84 52 74 71 67 64 88 68 71 75 66 57 61 62 52 47 50 58 60 64 66 57 46 54 66 80 80 68 71 87 64 77 66 83 77 58 46 41 43 56 55 51 56 56 54 \n",
      "45 52 39 46 56 45 39 47 48 40 39 50 44 69 70 67 72 59 51 52 51 48 52 53 64 75 81 85 98 110 107 130 127 109 101 80 93 94 76 58 81 78 83 78 72 77 60 74 61 75 79 74 64 93 67 69 70 53 51 50 59 55 51 47 47 47 61 61 62 47 44 64 75 78 58 66 82 81 86 73 70 68 57 47 38 39 39 51 53 52 50 51 \n",
      "45 50 42 51 51 45 40 48 44 37 36 37 44 49 52 43 56 51 47 44 46 61 72 77 79 85 100 107 111 122 144 118 123 113 111 85 80 80 83 69 62 73 100 83 86 80 73 76 67 69 80 87 79 85 68 72 56 55 47 43 45 49 43 52 57 64 71 59 55 47 45 62 80 76 80 80 91 88 94 82 70 63 54 52 47 41 33 49 51 48 53 50 \n",
      "49 46 47 47 50 47 42 45 40 44 61 61 49 65 62 61 54 67 77 61 53 55 57 64 68 62 71 97 112 89 113 133 113 103 81 94 74 73 75 75 64 75 95 96 93 91 80 79 73 68 83 82 86 86 67 75 58 53 49 55 56 53 53 55 58 65 57 58 53 48 50 62 90 87 89 95 92 88 90 91 84 79 72 57 46 39 35 31 43 43 50 51 \n",
      "46 46 47 48 48 44 43 44 60 54 50 69 72 66 63 62 48 65 57 54 39 59 74 69 64 77 84 82 88 97 92 90 123 95 83 68 77 69 71 72 71 77 95 101 105 92 79 88 79 82 86 82 92 84 78 73 58 56 61 68 76 81 85 93 83 60 46 49 47 48 56 61 96 86 85 90 87 82 83 70 81 81 72 64 43 46 31 34 38 41 53 48 \n",
      "47 45 48 51 44 35 41 49 56 62 56 71 68 65 67 50 36 55 43 44 54 67 71 71 71 86 90 93 70 67 80 82 84 116 75 81 69 81 68 79 77 83 98 112 115 88 81 102 98 84 101 88 99 94 86 76 60 65 74 93 92 93 94 99 104 90 75 65 51 59 57 60 85 84 80 64 81 72 76 69 65 69 60 53 47 39 34 33 39 43 51 53 \n",
      "45 48 46 50 33 37 42 51 60 61 68 61 60 56 58 41 34 36 39 63 85 81 62 58 69 76 83 72 69 57 69 85 78 90 94 86 69 73 80 79 92 88 113 121 106 96 95 115 100 92 102 89 111 91 84 70 62 74 95 99 101 93 95 97 92 97 85 71 68 71 59 63 77 87 77 68 79 72 55 58 66 68 66 53 45 46 36 35 32 46 53 48 \n",
      "47 49 47 55 40 31 54 52 65 60 71 59 44 43 53 36 39 43 71 81 70 63 52 60 88 83 86 75 67 57 67 92 90 75 98 96 86 84 86 84 101 88 114 127 102 99 106 112 100 101 93 106 117 91 81 77 69 74 85 104 115 104 101 93 89 83 83 86 84 78 70 59 72 93 84 72 71 64 59 60 65 59 49 53 54 53 42 33 37 34 51 49 \n",
      "48 48 50 57 37 51 62 46 48 55 64 50 41 36 43 34 37 46 77 97 84 66 68 86 91 83 71 92 77 68 78 73 107 86 83 103 86 97 89 102 98 105 107 117 109 112 118 110 112 97 95 112 108 101 100 90 88 81 84 96 101 101 104 99 91 92 92 86 80 79 71 63 68 84 70 66 67 63 62 56 62 49 60 52 60 52 64 44 36 39 45 49 \n",
      "53 46 50 55 41 57 46 59 71 52 60 44 44 41 37 35 40 57 85 92 71 76 78 84 79 79 78 82 108 95 96 97 103 99 93 91 104 91 95 101 102 107 112 114 115 130 131 118 118 110 113 124 124 119 114 104 95 86 78 82 92 101 99 99 93 83 88 91 88 73 67 63 65 81 79 72 61 56 45 56 53 47 50 64 60 51 51 61 48 39 50 51 \n",
      "52 48 49 55 39 46 56 89 49 55 45 51 36 35 41 44 54 54 70 72 63 70 66 79 86 98 99 98 105 113 122 122 114 110 110 98 102 109 107 112 121 131 125 126 134 133 132 130 125 124 134 144 142 133 127 121 107 106 96 92 88 100 96 109 91 85 85 96 89 84 78 82 76 81 83 76 73 64 61 54 44 45 54 56 58 48 47 46 49 39 45 50 \n",
      "50 49 54 46 36 50 57 40 40 60 50 54 43 41 41 42 51 77 92 81 76 78 94 100 103 110 121 129 124 123 129 129 129 132 121 120 127 123 127 129 139 143 149 147 144 147 147 147 144 143 156 163 162 157 145 138 125 123 120 120 109 106 97 96 100 81 80 89 93 95 82 83 77 85 86 79 75 62 68 65 46 37 43 49 54 61 47 40 48 36 52 44 \n",
      "53 47 58 37 45 48 49 52 54 51 62 56 56 43 51 53 61 87 96 85 85 96 110 116 125 127 131 136 136 141 140 141 139 138 135 133 136 130 136 141 144 156 165 163 155 156 158 160 157 160 161 180 166 163 160 159 147 138 133 128 126 120 116 106 100 84 82 82 97 98 80 83 75 79 78 77 68 74 59 56 61 43 47 57 38 52 67 49 46 35 46 48 \n",
      "52 49 59 39 49 47 44 71 54 51 58 56 60 62 58 71 79 101 91 90 107 114 124 128 132 136 135 135 138 139 141 145 144 146 144 143 145 141 146 153 156 169 171 169 166 168 165 165 166 166 169 179 172 165 168 176 163 154 143 137 132 131 129 123 118 90 87 93 95 100 93 79 74 72 84 74 67 60 61 52 60 57 64 61 49 42 60 54 51 34 41 50 \n",
      "55 47 64 42 49 48 50 53 57 61 59 57 62 70 76 86 104 102 93 116 119 126 129 132 132 135 133 137 137 138 137 146 148 142 145 148 147 150 154 166 167 176 178 170 176 183 178 168 168 172 172 186 186 173 172 172 171 160 154 152 141 132 134 135 133 113 90 103 100 95 93 84 72 88 74 68 62 62 49 53 62 56 78 64 53 42 58 42 51 31 44 48 \n",
      "46 53 51 47 59 45 51 40 64 79 66 55 61 77 89 101 113 106 113 125 123 135 131 130 133 133 137 136 139 139 142 145 145 145 148 153 155 153 168 175 176 189 182 172 187 186 185 172 174 184 185 191 190 187 186 185 176 165 167 161 158 148 134 133 136 138 105 105 95 92 81 88 78 89 66 68 73 54 50 48 49 66 82 59 58 41 52 40 32 43 42 47 \n",
      "49 56 56 74 73 73 45 39 43 56 53 48 71 79 83 95 114 122 128 125 128 134 128 131 136 133 142 140 143 144 143 147 146 147 159 161 159 165 178 185 189 186 186 179 192 189 183 178 179 183 188 197 199 199 197 195 195 188 185 167 160 159 148 139 137 136 129 107 89 84 94 95 98 78 81 85 69 72 69 51 68 69 66 65 71 40 37 39 28 45 45 47 \n",
      "51 69 65 64 43 81 57 43 49 50 54 52 58 78 87 107 125 126 131 128 133 134 134 138 143 145 147 150 149 149 149 152 153 157 167 165 165 174 182 183 183 182 188 191 198 192 188 184 182 184 186 191 196 193 195 199 200 193 186 178 167 163 157 153 146 141 134 125 95 81 88 101 97 93 80 81 65 56 68 69 62 63 61 55 64 38 38 34 28 45 49 48 \n",
      "50 62 50 53 46 78 63 63 51 58 54 66 85 104 114 127 128 133 134 133 140 139 141 144 151 154 152 158 156 158 158 160 160 162 171 169 172 180 178 180 182 183 187 194 194 193 187 180 180 183 187 188 190 190 184 195 183 188 184 182 178 170 167 162 156 147 141 133 119 102 109 108 109 110 94 87 75 61 68 68 58 68 42 56 67 43 33 39 27 44 50 51 \n",
      "54 46 52 48 42 79 58 65 64 64 52 77 104 119 130 137 136 134 135 136 145 145 146 153 156 159 159 161 159 159 161 165 168 168 171 172 170 179 177 181 183 179 185 183 186 186 180 183 179 175 180 183 180 184 180 187 181 179 179 178 175 170 172 167 162 158 148 135 133 129 126 123 123 120 106 89 88 71 74 94 51 39 45 42 72 59 47 30 38 46 48 48 \n",
      "54 52 51 46 40 61 52 66 71 66 73 71 102 123 140 140 144 142 145 143 150 148 156 159 157 164 162 164 162 161 158 168 170 171 170 171 169 176 173 181 184 183 181 181 182 181 179 182 176 174 177 177 178 177 174 173 176 175 175 177 174 168 172 171 171 166 166 153 140 142 144 142 135 125 121 105 92 81 77 69 78 42 38 37 49 59 49 26 44 50 49 48 \n",
      "62 53 49 39 42 42 53 57 69 66 74 75 103 127 140 146 147 145 146 150 150 152 160 160 160 162 166 165 167 165 162 165 169 168 169 176 173 176 174 175 177 178 176 182 180 178 178 178 176 174 177 176 175 176 179 177 176 176 175 177 176 173 172 174 167 172 168 165 163 162 155 153 148 134 122 105 93 76 68 70 66 54 39 40 35 45 55 31 50 50 47 48 \n",
      "60 54 40 35 38 37 57 59 77 72 78 88 108 130 137 145 148 150 147 152 155 156 159 163 163 166 167 168 167 167 166 168 166 167 170 175 171 174 176 177 180 183 178 184 184 183 180 176 176 172 175 176 172 174 173 175 176 173 176 175 173 176 175 172 172 171 172 167 168 168 168 163 159 139 134 117 90 86 68 68 72 60 43 48 41 33 55 30 47 50 51 49 \n",
      "68 49 47 34 35 48 51 56 71 70 89 88 107 124 143 147 149 150 153 154 157 158 161 163 167 166 169 168 169 164 165 170 169 168 168 172 171 173 178 178 177 179 178 179 179 182 177 177 179 175 174 173 173 173 173 174 177 178 173 175 173 174 170 168 170 169 168 168 167 166 166 164 160 153 135 121 97 89 67 76 79 62 51 54 50 36 44 35 44 53 45 49 \n",
      "60 45 41 31 42 54 62 56 64 82 84 91 109 130 142 149 148 153 151 157 158 161 159 163 167 168 171 172 168 170 165 170 172 171 170 171 173 172 177 184 180 184 180 183 186 182 181 179 179 178 171 172 174 174 175 171 174 174 176 173 175 174 173 171 169 170 165 166 169 163 165 164 155 152 139 121 108 90 72 73 72 65 50 59 53 42 34 39 45 52 44 50 \n",
      "59 41 32 38 58 50 65 57 77 91 93 98 112 131 144 148 151 152 155 160 160 161 163 168 167 169 169 171 170 171 170 167 171 173 172 173 176 176 177 181 184 182 183 185 185 182 182 182 181 177 175 176 175 175 176 174 174 172 170 173 173 177 174 171 171 168 168 165 171 168 163 162 154 153 138 121 97 102 74 72 64 58 57 47 48 39 57 40 42 44 47 48 \n",
      "61 42 36 39 44 45 67 65 70 94 90 99 113 130 140 146 155 158 159 162 160 162 164 167 169 171 170 170 171 170 171 167 172 172 176 174 176 180 179 190 187 182 185 186 187 185 183 180 178 175 174 175 172 175 174 170 174 172 173 174 171 175 170 173 171 169 169 169 165 172 163 160 156 154 142 116 101 91 81 67 86 65 59 46 42 36 65 49 36 51 45 46 \n",
      "49 50 47 41 36 47 51 66 80 81 84 99 112 128 142 149 155 157 159 163 164 159 164 169 172 171 169 168 172 171 168 171 171 172 174 171 174 178 179 185 184 180 184 185 189 185 182 181 174 173 175 174 174 174 174 171 174 176 172 173 171 172 171 171 170 170 170 171 168 167 164 160 155 150 140 123 99 94 71 72 77 86 79 53 39 39 40 47 45 44 48 47 \n",
      "51 50 64 35 41 45 70 66 74 82 93 102 111 127 142 151 159 160 162 165 167 164 167 170 170 170 167 170 174 172 170 170 171 173 178 176 174 175 180 183 181 178 184 186 186 184 183 179 173 176 176 173 176 171 175 172 172 173 176 173 173 169 170 173 174 170 170 170 167 166 165 161 157 153 136 126 103 90 68 62 73 80 70 48 56 38 37 33 46 47 50 47 \n",
      "46 50 50 35 56 53 74 76 74 83 88 95 109 125 142 154 159 163 164 163 166 169 168 171 171 165 169 169 169 170 169 171 169 168 172 172 177 179 186 185 185 182 182 183 185 185 182 178 178 177 177 171 173 172 176 174 171 171 175 173 173 171 172 171 173 170 172 170 170 168 167 161 154 149 135 128 104 92 73 78 66 69 51 45 48 49 31 29 44 47 45 47 \n",
      "38 51 38 42 61 54 67 67 85 88 96 98 115 136 146 158 160 161 169 167 165 170 166 169 168 166 168 173 170 172 174 173 168 169 171 176 174 179 185 189 185 181 183 184 186 188 185 179 180 178 177 176 171 172 175 175 175 172 175 175 175 171 171 178 171 172 173 171 170 172 169 160 157 148 137 121 109 97 83 77 77 60 42 44 42 41 36 34 47 49 45 47 \n",
      "42 56 33 43 50 50 59 68 86 94 99 108 132 142 153 159 167 161 167 168 165 168 167 169 168 163 168 170 173 175 175 172 171 174 174 176 181 181 189 188 187 185 186 188 183 184 184 179 180 181 177 181 174 174 172 172 175 173 176 176 179 173 175 174 174 173 171 170 171 169 164 163 156 153 132 129 109 88 82 74 66 53 41 42 38 40 38 35 42 50 48 48 \n",
      "55 54 29 48 53 51 63 63 82 88 103 106 125 142 157 162 166 168 165 171 169 168 170 167 166 169 166 169 170 170 167 171 170 172 175 176 183 189 187 189 187 179 183 190 185 188 186 180 178 180 180 177 174 175 174 176 173 174 173 174 176 173 177 176 173 175 175 171 170 170 166 159 160 152 148 122 108 95 81 86 74 55 41 46 36 41 38 35 50 50 48 46 \n",
      "60 59 48 43 41 59 73 67 89 94 106 110 131 146 160 163 168 166 165 168 166 168 170 168 170 166 172 172 172 172 174 168 171 175 176 178 184 187 190 190 188 191 188 192 193 187 188 183 181 179 177 179 173 173 170 175 172 176 177 179 176 177 175 173 176 176 173 174 175 166 167 161 157 155 144 139 106 95 86 76 66 72 51 38 42 42 34 37 50 47 47 50 \n",
      "67 55 42 41 37 51 56 74 95 104 109 118 133 149 162 166 168 167 168 168 168 169 171 171 170 170 171 173 172 173 174 174 172 178 178 178 185 190 192 189 188 187 196 193 190 192 181 187 189 177 178 175 174 176 176 174 174 174 174 176 177 175 176 175 176 173 177 174 174 167 165 163 156 157 150 131 118 94 90 78 53 53 53 37 56 40 42 39 50 50 47 45 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 47 38 37 45 42 66 76 88 107 110 127 139 156 164 165 167 168 167 170 166 170 171 170 173 169 171 171 172 173 172 175 177 177 177 180 184 189 197 198 193 188 194 192 192 193 188 187 187 180 175 174 169 176 173 175 174 172 177 172 177 177 175 179 177 175 175 175 173 172 167 164 159 155 156 132 125 95 94 85 77 59 49 39 42 39 42 35 54 50 48 46 \n",
      "61 50 34 43 70 41 53 64 80 106 116 127 142 156 164 162 165 166 166 167 166 171 174 171 174 171 173 171 173 170 173 174 176 177 179 182 186 184 195 197 193 196 194 191 183 189 188 187 186 181 176 175 170 172 172 175 173 175 176 175 176 176 175 177 174 177 176 172 173 171 167 164 157 156 156 144 119 96 88 70 106 57 67 41 38 38 36 31 46 48 49 45 \n",
      "57 63 23 62 82 37 50 70 88 107 116 140 155 160 163 158 165 164 163 164 166 169 171 170 173 173 176 176 175 171 174 172 176 176 181 182 182 187 195 192 194 197 190 195 191 191 188 185 184 177 175 176 170 168 168 175 170 170 175 174 175 176 177 178 175 174 175 178 172 173 166 166 157 156 151 148 127 101 81 82 77 54 54 51 40 36 38 34 47 49 46 44 \n",
      "46 63 31 45 74 32 57 77 74 101 113 145 159 155 162 160 160 164 159 163 164 166 172 168 172 171 175 177 174 172 173 174 173 177 183 181 182 191 190 194 193 195 191 197 191 192 191 184 188 182 175 171 170 165 168 166 167 166 171 172 169 175 175 174 175 175 176 177 176 169 167 162 160 154 152 144 139 103 70 95 64 49 44 43 45 44 39 46 45 49 46 46 \n",
      "47 55 38 33 61 42 54 84 74 96 129 157 161 156 157 159 155 161 161 166 166 168 167 168 172 172 169 173 172 173 173 175 175 177 179 178 183 191 191 191 188 189 191 192 188 192 189 185 182 177 174 171 167 166 167 163 165 165 169 171 170 174 169 172 176 179 174 177 179 172 168 163 159 157 149 144 142 109 78 87 57 50 37 46 39 50 45 44 53 45 50 49 \n",
      "50 52 41 31 41 52 51 76 68 92 131 160 158 154 153 158 154 161 164 162 160 163 167 167 171 173 169 172 171 171 174 176 176 176 178 178 181 183 187 186 188 186 187 187 185 185 182 181 181 176 173 171 171 168 164 166 164 167 169 168 168 172 167 166 171 173 176 170 175 172 172 164 159 155 150 144 143 110 84 90 55 51 41 44 45 47 48 45 52 43 48 47 \n",
      "53 55 37 30 50 69 52 64 56 92 139 155 155 153 150 152 159 162 162 149 154 162 167 170 167 171 170 167 171 167 170 173 169 174 178 174 178 180 183 180 182 182 184 179 177 183 180 179 180 177 173 171 171 170 167 166 168 164 165 162 165 164 162 161 167 163 166 162 163 170 170 173 159 153 149 145 141 108 97 71 47 52 52 37 53 45 47 49 49 42 49 49 \n",
      "49 55 46 23 67 78 65 54 64 83 143 154 154 149 151 153 161 161 154 136 152 160 163 159 166 163 163 158 161 162 164 159 167 171 173 171 176 172 179 181 178 181 180 178 176 180 178 179 184 178 173 167 171 167 163 156 150 149 140 145 144 147 153 149 152 159 154 156 151 159 161 164 161 155 145 148 143 120 79 62 45 37 50 44 52 53 47 49 55 49 52 48 \n",
      "51 52 43 33 48 95 67 41 71 75 143 151 151 147 152 157 159 156 144 139 149 152 156 150 147 144 139 138 136 140 155 148 155 167 167 170 171 171 172 180 178 177 182 179 181 179 181 182 178 181 174 169 166 156 138 127 120 120 122 121 128 121 133 136 135 145 151 151 141 151 152 150 153 159 147 143 150 134 92 48 38 38 50 68 44 48 54 50 51 53 46 49 \n",
      "53 50 53 30 43 95 61 39 76 76 145 148 150 145 153 156 153 153 149 147 149 148 151 144 132 130 126 125 133 135 139 149 150 160 163 167 166 166 174 178 177 180 183 181 177 173 178 179 179 174 168 170 161 149 134 127 117 108 107 112 119 113 119 125 131 142 145 147 143 145 139 141 151 153 148 146 148 142 100 48 36 35 62 67 58 50 64 52 49 50 48 52 \n",
      "52 47 55 33 42 88 57 45 76 80 142 147 147 148 151 146 148 145 151 158 146 142 146 142 126 121 117 116 121 124 133 140 138 152 158 161 162 163 173 176 178 178 183 181 181 168 177 175 175 173 165 160 159 151 133 130 129 126 117 124 132 138 142 132 137 151 155 154 155 150 143 138 144 143 143 144 147 153 98 72 35 45 63 54 55 54 46 77 45 49 49 51 \n",
      "54 50 58 38 37 68 64 44 73 84 142 150 150 148 147 142 144 146 149 163 157 148 150 150 147 147 151 142 140 140 134 144 149 159 161 159 166 172 165 175 180 180 179 180 182 169 173 173 177 174 170 159 159 155 148 145 145 145 143 147 152 154 157 154 155 161 169 175 176 167 157 148 140 135 141 143 149 155 109 86 52 49 53 56 50 56 46 68 56 44 47 49 \n",
      "54 52 49 60 35 72 61 31 84 83 138 150 148 145 143 140 153 152 159 169 174 171 170 164 161 157 165 164 156 154 154 154 160 162 164 169 175 179 181 186 188 182 178 180 180 170 170 173 178 178 174 172 166 162 155 149 143 139 134 135 129 134 138 149 152 156 163 174 178 176 165 157 146 136 143 141 146 154 137 80 60 65 55 54 49 55 86 86 66 41 47 49 \n",
      "54 81 100 75 37 75 46 33 82 85 142 147 148 141 145 152 155 169 178 187 183 180 175 169 171 160 156 147 131 125 125 132 145 154 155 162 173 181 182 191 192 186 177 179 182 171 169 168 169 171 172 175 168 159 144 126 113 97 108 108 65 86 67 73 79 87 118 132 148 165 171 161 146 145 142 145 148 148 155 93 53 74 65 49 67 125 174 158 144 66 45 43 \n",
      "113 143 139 127 81 64 45 43 81 100 144 148 149 152 152 158 161 183 196 188 183 170 158 142 121 92 99 89 77 74 96 94 108 126 144 148 163 174 184 186 184 181 178 175 181 171 167 171 163 165 166 168 163 143 116 115 113 122 165 143 86 123 140 41 73 45 118 103 84 126 155 160 153 147 150 147 152 150 154 107 53 69 60 72 123 155 181 165 166 150 44 45 \n",
      "167 161 161 155 130 76 65 71 74 108 146 151 153 161 157 169 176 183 186 166 138 106 119 89 89 63 159 85 98 71 142 120 100 104 114 142 160 178 174 180 178 178 178 170 181 174 168 166 163 157 159 158 161 133 107 111 131 144 177 158 85 81 95 51 115 67 156 170 99 78 113 149 152 148 154 151 154 150 157 120 72 64 79 113 159 174 176 155 151 188 95 35 \n",
      "177 160 172 178 166 140 89 107 76 121 151 154 159 167 164 173 179 176 155 107 101 133 174 126 74 81 54 90 91 77 147 157 119 93 118 121 168 165 162 170 174 178 174 166 182 177 173 163 161 150 147 152 142 122 132 116 142 149 168 183 115 98 102 71 175 137 180 166 124 105 100 124 153 151 159 154 154 152 154 140 75 75 99 151 169 178 156 130 169 153 184 32 \n",
      "138 160 154 189 186 174 148 127 77 130 152 156 164 170 176 167 178 151 125 83 136 164 186 190 132 97 78 92 90 107 161 156 136 121 129 97 148 161 149 162 171 177 166 163 179 184 184 175 161 146 140 142 131 116 120 115 120 134 146 174 183 129 108 91 142 171 157 134 133 124 119 117 147 157 165 159 156 155 152 154 88 92 125 167 175 155 152 147 171 140 198 80 \n",
      "128 158 172 162 191 188 181 165 84 141 154 161 168 173 178 178 165 145 131 132 122 137 234 223 229 103 99 97 109 134 143 125 139 121 125 112 140 146 142 154 164 168 165 161 172 190 183 189 173 151 139 130 125 139 141 146 134 139 136 139 158 139 132 125 124 138 129 136 136 131 124 119 141 159 168 163 159 158 150 153 124 123 154 181 138 151 170 174 152 149 158 176 \n",
      "121 153 174 175 137 191 192 186 135 138 153 162 169 173 184 189 164 143 138 151 134 104 137 187 165 137 124 138 136 133 141 137 134 146 144 130 131 133 142 157 161 165 161 164 174 191 173 187 178 159 139 126 117 121 143 152 140 134 140 147 136 134 135 128 127 142 156 145 148 140 140 132 138 158 164 165 162 161 154 153 131 138 168 166 121 161 173 178 164 144 129 210 \n",
      "112 154 173 173 172 129 198 192 149 142 151 164 168 174 186 184 159 144 150 157 162 137 134 135 136 134 132 147 152 146 137 139 142 148 145 128 131 131 145 156 162 164 162 161 172 192 174 183 176 160 146 136 127 115 125 134 141 140 137 135 140 143 148 150 154 165 176 163 151 152 151 153 151 158 164 167 163 164 154 153 134 149 177 135 119 154 169 177 174 135 122 187 \n",
      "114 159 163 173 168 133 172 198 149 143 154 166 172 173 177 170 156 156 160 166 174 179 168 160 157 155 152 153 147 139 145 146 145 142 135 133 137 140 148 159 163 164 166 160 173 194 176 176 178 163 157 143 141 126 120 136 143 142 141 141 144 147 148 155 163 171 173 171 158 158 158 158 159 163 168 164 165 164 158 149 138 149 170 143 140 164 172 172 181 134 124 155 \n",
      "130 163 177 182 185 175 171 196 150 143 159 166 174 170 167 162 157 161 163 174 184 196 195 181 170 166 158 153 149 149 144 150 144 134 135 141 145 151 156 163 165 163 163 162 167 180 177 173 177 167 162 146 140 142 131 126 141 148 147 143 147 149 148 151 161 165 167 164 162 160 161 160 162 164 164 166 166 167 159 148 138 157 120 168 180 179 172 172 181 154 124 143 \n",
      "151 174 181 181 195 190 136 149 153 146 159 165 171 169 165 161 161 164 165 173 183 191 189 181 175 165 159 156 151 152 146 146 144 140 143 152 154 157 160 164 170 167 162 164 164 171 171 169 174 168 162 152 143 145 145 141 132 142 145 147 149 150 151 149 152 150 159 157 164 159 164 161 165 162 165 164 166 162 160 148 143 133 80 90 125 154 158 165 175 182 125 146 \n",
      "163 175 169 164 153 121 113 94 153 149 157 165 170 171 167 161 160 160 162 172 172 176 171 168 168 161 160 154 155 156 153 148 146 150 155 155 162 164 167 169 167 166 160 165 166 171 173 164 170 168 164 162 150 148 146 149 144 140 145 149 151 153 150 148 155 152 155 157 160 162 161 163 166 165 165 167 166 162 158 148 148 126 85 122 112 118 139 152 166 188 128 164 \n",
      "166 171 156 136 123 116 135 105 139 148 157 164 169 171 168 164 162 160 165 168 171 170 165 168 165 162 161 164 158 161 155 154 156 160 161 160 166 168 168 167 166 165 163 164 169 173 179 173 171 168 164 164 154 146 148 153 150 150 147 147 152 158 154 157 157 155 156 156 161 164 163 164 165 169 167 169 166 162 158 146 142 149 82 139 132 114 125 139 157 189 132 186 \n",
      "175 168 148 118 117 137 140 127 136 149 155 162 170 171 170 163 164 161 166 167 164 169 166 167 162 164 164 165 163 157 158 161 161 166 168 164 167 167 170 171 168 165 166 168 168 174 178 180 174 172 164 169 159 148 149 153 152 155 156 152 151 155 152 160 157 155 160 159 157 162 166 165 170 169 169 171 166 161 157 147 140 152 112 133 146 123 123 134 159 186 142 192 \n",
      "176 164 149 121 121 163 129 151 143 147 153 162 171 172 167 167 165 165 165 166 165 164 168 169 169 166 164 160 160 158 162 167 164 168 170 169 169 171 169 169 169 170 168 172 169 171 176 175 174 174 168 168 164 155 146 155 159 157 159 159 155 153 155 156 156 158 157 160 161 162 166 168 171 170 169 170 169 163 154 144 141 149 138 112 140 126 128 138 163 180 160 178 \n",
      "173 166 151 121 125 147 125 172 148 145 155 163 169 170 165 172 170 165 170 170 165 170 166 168 164 159 158 158 160 159 162 166 167 167 170 173 173 176 171 170 170 168 165 163 167 173 175 179 177 174 170 171 168 159 156 153 158 156 160 163 161 162 158 159 161 162 164 164 166 169 167 169 169 171 169 171 168 164 152 143 138 151 134 108 133 122 128 149 179 165 176 134 \n",
      "161 177 164 131 123 145 117 187 155 146 151 166 169 172 172 176 171 172 172 176 170 166 171 165 163 165 167 160 162 167 166 171 171 170 180 180 178 170 166 166 168 161 157 162 167 179 179 180 181 178 172 172 168 157 166 163 164 159 161 160 163 164 161 161 163 167 166 168 167 169 170 172 168 171 172 171 167 164 151 142 139 145 126 110 128 119 159 172 183 156 187 98 \n",
      "154 190 181 167 136 140 109 176 167 147 148 167 168 174 175 180 174 178 174 177 169 170 171 169 169 170 168 170 168 170 168 173 172 180 183 188 171 156 157 157 163 157 147 159 166 180 182 178 180 175 171 174 166 147 168 171 172 162 161 163 162 167 162 161 168 168 170 168 168 172 170 172 168 172 169 169 162 163 149 147 144 131 136 171 166 160 182 183 168 172 165 60 \n",
      "178 166 196 197 171 156 144 111 178 149 144 164 169 181 178 181 174 178 178 175 169 172 170 172 174 172 169 171 171 172 169 174 179 184 190 183 156 149 149 151 159 160 161 162 171 180 182 177 176 172 169 170 170 155 161 178 174 165 168 157 164 163 166 169 170 171 169 169 170 172 171 169 167 167 168 169 161 160 149 146 142 126 142 181 184 175 182 174 166 186 119 40 \n",
      "189 162 166 192 181 162 173 125 145 150 149 161 172 183 182 181 176 179 174 177 171 172 170 176 174 177 175 175 170 173 175 177 180 187 186 170 151 145 148 154 161 161 162 166 180 181 182 180 174 175 168 168 169 167 159 173 176 175 172 166 167 167 168 170 168 172 170 168 171 171 170 167 170 169 165 165 165 154 148 143 141 134 160 179 184 176 165 161 182 158 57 45 \n",
      "122 193 167 160 167 174 181 174 129 150 150 160 171 183 184 181 181 178 173 178 177 174 177 176 177 176 170 175 172 176 177 179 181 183 179 166 151 151 154 161 160 164 165 168 177 181 179 175 178 172 170 169 167 164 164 165 176 178 172 172 173 166 168 173 171 172 170 170 174 170 168 167 170 172 168 163 163 155 146 145 135 145 165 178 180 171 160 174 184 94 36 46 \n",
      "22 177 191 167 170 187 187 188 170 143 152 157 170 179 188 180 181 183 179 180 177 179 180 176 179 171 169 172 175 177 177 180 178 179 175 165 152 155 166 168 166 171 168 165 177 182 175 177 177 175 173 170 163 161 164 161 172 176 172 172 174 167 170 170 171 176 175 174 176 172 169 171 173 171 167 162 164 155 145 148 136 143 171 177 179 165 169 179 123 37 47 48 \n",
      "39 63 182 173 178 183 188 188 173 141 154 158 168 177 181 184 179 185 182 182 183 180 183 178 180 170 174 174 173 176 175 178 178 176 172 162 160 163 172 176 175 178 178 170 195 205 183 175 176 175 170 168 164 159 158 161 168 174 175 174 178 175 173 174 171 175 175 179 179 173 173 171 173 171 165 164 165 155 144 144 134 144 161 168 177 160 161 114 57 40 46 46 \n",
      "44 35 62 149 172 184 176 171 168 147 152 154 166 178 180 183 182 187 181 186 183 184 186 181 182 173 181 174 174 177 174 175 177 174 173 166 168 178 183 183 178 179 193 179 180 192 177 182 185 166 160 168 167 166 160 156 161 174 176 175 175 175 175 176 177 175 178 180 180 175 174 173 171 171 167 163 162 153 143 144 133 160 167 173 171 130 82 58 40 44 46 44 \n",
      "49 51 36 38 114 189 186 174 177 163 150 151 163 177 177 182 180 188 183 191 186 190 186 179 183 177 178 177 176 174 173 171 177 173 171 166 178 195 188 170 169 168 195 199 180 181 177 186 177 157 152 157 176 173 161 151 159 175 176 176 175 177 172 179 179 180 183 180 183 181 177 176 172 171 167 163 162 151 143 139 144 165 172 161 123 62 41 48 46 43 44 44 \n",
      "53 44 47 45 39 109 176 189 194 196 142 154 160 172 181 184 181 186 190 192 191 188 190 184 181 178 181 178 177 173 172 172 173 175 173 154 180 191 153 119 141 169 172 193 188 185 180 164 155 142 122 94 169 176 159 135 165 176 180 175 177 177 175 179 179 184 182 181 184 180 177 179 174 174 168 164 160 146 145 134 144 140 118 84 60 45 44 45 45 43 43 47 \n",
      "47 49 48 43 43 39 63 122 175 192 147 153 157 171 179 185 182 189 191 193 188 189 186 185 186 179 179 180 178 174 171 173 175 177 178 142 135 157 146 143 165 161 166 177 183 183 175 162 153 143 143 121 133 153 127 140 172 179 181 178 177 179 178 177 180 179 179 181 185 178 177 179 174 171 169 162 155 144 147 134 72 59 57 47 45 41 41 47 43 45 42 43 \n",
      "47 48 40 46 43 48 40 43 61 72 95 151 160 171 176 183 186 190 187 193 186 186 185 188 187 184 184 182 179 177 174 176 178 182 180 166 140 143 150 156 167 165 155 167 179 180 172 149 145 150 145 139 130 135 141 161 178 180 181 177 178 181 177 183 180 182 180 185 186 183 174 185 170 170 169 164 155 145 152 115 40 44 42 45 41 43 42 45 43 43 45 45 \n",
      "42 43 45 43 43 43 45 40 45 44 32 152 157 174 170 184 186 191 191 190 191 190 190 191 190 185 188 183 180 177 180 177 173 178 177 163 151 160 164 165 169 172 168 168 180 177 159 149 154 159 151 144 148 157 157 169 184 183 184 182 182 182 176 183 183 186 184 185 185 184 179 182 170 170 171 162 151 143 157 85 32 43 44 42 44 45 43 44 45 45 42 46 \n",
      "45 45 41 45 42 46 44 43 46 43 17 131 158 168 173 183 186 190 190 190 195 191 195 191 193 188 185 180 180 176 180 179 175 168 175 159 151 154 160 165 167 173 172 179 182 174 162 164 159 161 151 147 151 156 162 168 183 186 184 185 186 183 173 179 182 184 188 180 183 184 181 179 175 174 171 161 149 146 145 60 36 40 43 44 44 44 40 42 44 43 44 47 \n",
      "48 42 44 44 43 44 45 43 47 40 28 76 163 165 176 183 187 190 190 194 195 194 194 194 194 189 185 183 182 179 178 177 175 171 170 160 153 153 156 157 168 168 168 172 180 172 164 165 155 155 150 146 151 162 165 167 183 185 183 186 187 185 180 181 182 182 186 180 183 184 180 174 174 174 169 157 148 155 122 38 39 45 42 38 42 47 39 46 42 44 42 44 \n",
      "46 44 43 43 44 40 44 43 40 47 45 38 164 162 172 181 187 191 191 195 196 198 192 193 192 190 187 184 182 182 174 174 171 171 167 159 152 152 152 159 167 165 167 169 178 170 168 157 153 155 151 147 153 164 162 169 177 184 184 186 187 186 183 185 186 179 186 186 186 184 175 176 174 175 165 154 143 159 88 35 39 43 42 45 39 45 39 46 38 42 46 43 \n",
      "49 43 43 46 45 43 41 43 41 45 46 14 148 161 167 180 187 191 194 197 197 198 195 194 192 190 188 188 178 177 168 172 166 169 168 162 159 160 161 162 164 163 164 175 178 170 171 162 153 152 150 161 158 159 162 160 174 179 183 184 183 184 183 186 185 183 184 185 185 182 176 177 172 174 165 153 148 148 58 36 43 42 42 41 46 37 40 44 41 43 43 38 \n",
      "46 47 45 43 44 38 43 43 43 42 48 20 106 161 170 171 185 190 192 198 195 198 198 199 192 190 186 185 177 176 173 171 167 169 171 167 168 168 163 166 162 162 159 163 168 168 169 163 158 155 151 165 161 153 157 163 168 173 179 178 183 179 178 186 185 185 186 186 186 176 177 178 174 173 160 150 152 125 41 34 44 43 40 45 43 37 41 41 47 39 46 41 \n",
      "47 46 45 44 41 43 39 46 44 42 44 35 72 161 167 172 181 189 192 196 196 199 200 199 192 192 186 182 180 176 177 174 171 175 177 172 174 172 165 165 159 151 149 144 148 150 157 160 156 161 160 169 167 165 159 171 164 173 171 177 179 182 177 187 184 183 186 182 183 175 175 174 170 168 159 150 155 96 32 38 41 39 40 41 43 42 37 42 45 44 44 41 \n",
      "50 44 43 43 44 42 44 46 43 42 48 44 29 155 165 173 181 188 192 193 196 199 200 198 194 190 190 182 175 182 180 184 187 183 181 178 179 176 167 166 162 151 146 141 138 142 147 155 153 161 164 166 169 167 169 173 169 177 176 174 178 180 179 180 185 183 188 180 182 175 176 171 168 162 160 148 148 66 35 40 36 42 37 37 43 40 40 40 37 43 48 40 \n",
      "47 44 48 47 43 40 44 43 45 45 41 46 22 122 161 172 179 187 192 193 196 196 199 199 193 189 191 182 184 188 186 189 191 187 184 184 183 181 172 169 163 162 154 146 146 145 156 160 156 162 166 166 170 173 172 176 174 179 179 177 179 180 179 171 182 185 182 181 181 178 177 171 165 163 158 156 125 44 32 38 35 44 39 39 40 40 36 40 41 41 44 41 \n",
      "49 49 47 44 48 40 45 45 40 42 44 46 32 68 166 171 179 189 188 192 190 197 199 196 193 191 193 188 189 188 189 188 194 194 193 186 185 171 165 157 158 154 148 149 149 146 146 148 148 152 155 166 171 173 177 178 177 182 180 180 181 182 179 174 176 184 182 180 179 175 174 170 166 163 154 156 104 30 39 40 42 37 36 39 42 35 42 42 39 40 44 42 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 49 47 44 43 46 42 48 38 44 42 45 45 20 150 170 173 181 193 191 192 196 199 195 193 192 191 193 186 187 177 173 178 183 174 156 147 147 142 140 142 147 140 136 140 137 140 143 144 144 139 142 148 157 168 175 178 176 177 178 180 181 181 178 173 181 181 183 177 178 172 165 166 158 146 149 93 31 36 38 38 44 36 43 40 42 36 39 41 38 44 40 \n",
      "49 50 48 46 47 44 44 43 40 45 45 43 45 31 97 168 172 184 188 193 194 197 197 197 195 192 195 191 180 173 157 139 141 145 135 123 121 115 107 95 98 113 112 111 110 117 126 119 120 114 114 115 113 119 125 144 155 157 155 163 171 181 180 181 177 181 180 182 178 172 171 162 163 160 139 141 88 32 40 37 42 36 45 37 37 44 36 39 38 39 44 42 \n",
      "49 46 50 48 41 47 43 45 44 43 43 45 47 43 27 165 172 183 188 188 195 195 195 194 197 198 196 184 167 152 128 116 110 117 117 112 105 104 103 96 103 101 98 94 85 85 93 86 84 85 81 88 94 92 91 103 108 114 123 140 156 168 181 180 177 180 179 180 175 173 169 162 164 149 130 140 84 30 40 39 39 38 42 40 40 39 39 38 39 41 42 44 \n",
      "45 50 53 49 46 48 44 41 46 45 41 43 46 40 30 101 169 183 190 189 195 194 197 195 197 199 195 179 165 149 151 143 136 132 128 131 128 131 122 120 124 118 116 114 106 112 109 109 99 103 96 99 106 100 100 108 114 119 129 143 156 161 178 179 179 181 175 177 173 171 163 162 158 142 127 141 85 33 38 39 37 40 40 40 36 43 39 41 38 39 43 43 \n",
      "46 51 47 51 40 51 41 44 42 47 40 41 48 44 39 29 157 175 184 190 192 192 197 196 200 199 196 181 169 162 170 173 168 157 146 142 143 143 138 135 136 134 129 127 121 123 139 130 123 120 118 117 124 119 123 125 134 142 146 157 165 159 174 174 178 181 178 174 172 165 161 162 149 134 130 142 86 33 37 37 40 40 40 36 39 38 42 36 41 40 41 40 \n",
      "50 49 50 46 47 48 43 48 45 44 45 43 46 41 41 23 91 170 181 189 188 193 192 196 200 199 192 190 179 176 176 183 185 184 173 160 152 151 150 147 146 144 144 144 138 132 137 136 135 132 128 131 138 139 140 149 156 161 168 164 174 174 174 180 179 177 174 171 168 161 159 158 140 131 132 143 92 29 33 40 41 38 46 40 39 40 45 39 40 40 39 44 \n",
      "49 53 47 48 51 48 48 47 47 43 42 44 41 45 40 44 26 159 174 187 188 191 192 195 201 201 196 195 193 191 180 183 185 191 189 185 174 169 167 159 152 149 148 148 149 141 138 135 133 136 136 143 152 152 159 165 170 174 172 174 177 180 178 182 178 177 173 166 160 159 153 151 131 137 133 147 92 28 36 40 39 39 48 37 46 43 41 45 35 45 40 44 \n",
      "50 52 47 47 53 50 45 47 48 41 45 46 41 42 45 42 24 133 166 182 185 187 192 193 200 200 203 194 196 198 188 184 184 187 192 188 184 180 183 176 170 165 164 160 154 154 149 148 146 149 151 154 158 157 165 168 175 172 176 176 178 181 173 178 177 177 171 162 155 153 151 137 132 139 134 152 96 29 35 41 34 42 37 40 43 41 43 41 41 44 42 45 \n",
      "50 51 46 49 45 54 45 48 47 47 45 44 48 43 42 46 21 128 148 171 179 185 190 192 199 200 200 200 194 201 194 188 186 187 189 190 182 183 183 183 182 175 174 172 166 173 170 172 169 170 168 164 161 164 170 172 176 173 180 179 176 176 171 175 174 171 163 158 154 150 142 130 137 139 137 151 99 32 39 38 38 41 37 39 42 41 37 41 39 50 38 47 \n",
      "48 48 47 56 43 56 46 50 48 48 50 49 43 45 38 47 29 130 153 157 170 181 185 193 195 199 195 196 193 197 198 192 189 195 190 193 187 183 184 178 179 176 171 169 165 169 173 174 177 172 173 167 168 168 176 175 179 176 182 180 174 172 171 171 170 161 158 154 149 143 134 129 142 144 139 156 107 28 38 36 41 39 41 38 40 40 42 41 41 44 41 46 \n",
      "50 49 50 50 50 53 48 49 50 44 48 50 50 45 46 47 22 113 158 149 162 179 180 189 191 196 195 188 187 191 195 193 190 196 193 193 190 189 187 180 177 175 173 163 164 165 169 173 176 176 173 172 173 175 174 173 178 174 182 172 169 167 165 165 168 155 157 150 144 137 125 138 148 142 143 156 104 35 35 35 36 38 39 36 42 43 38 45 39 44 49 42 \n",
      "48 52 50 48 52 48 51 52 50 45 49 51 47 47 49 45 28 114 155 155 154 169 177 184 187 191 197 187 181 190 194 192 189 192 192 190 190 191 191 185 179 174 174 165 168 169 167 172 171 175 171 173 175 176 177 178 175 174 172 172 170 171 163 166 158 153 152 148 141 130 128 145 150 143 146 160 106 28 34 39 34 38 38 39 38 46 39 44 41 45 51 43 \n",
      "49 52 52 49 49 53 49 52 46 46 53 47 49 51 46 44 24 109 159 154 158 155 168 178 177 185 190 188 185 191 195 192 191 191 189 187 188 186 186 184 179 175 173 167 164 167 164 165 167 169 170 169 174 173 173 176 171 171 167 170 171 168 163 157 150 150 146 144 134 123 138 147 146 148 149 160 105 28 35 38 37 39 40 40 45 39 40 46 40 46 49 40 \n",
      "48 52 48 52 51 51 47 50 46 47 50 47 43 48 49 46 26 105 165 160 159 157 159 169 177 177 182 189 188 185 191 193 190 192 191 188 186 184 179 183 179 175 172 166 164 163 165 163 163 164 164 168 165 171 167 172 169 171 169 168 166 164 158 155 150 147 146 134 122 132 142 150 153 150 153 164 101 26 38 40 39 43 39 40 42 38 43 40 42 46 46 48 \n",
      "49 52 49 52 46 54 45 49 49 51 49 47 47 51 47 51 29 100 164 165 156 166 158 156 168 177 180 187 184 188 189 188 187 185 187 187 183 182 179 178 175 171 166 164 162 160 161 156 160 164 159 165 166 167 172 169 170 173 169 164 160 158 154 153 145 142 138 128 126 143 148 153 154 155 156 160 105 27 38 36 41 40 45 40 45 38 39 44 43 47 47 45 \n",
      "48 50 49 50 55 44 53 50 51 48 52 47 48 53 46 53 19 104 163 167 165 164 172 152 153 166 172 176 186 182 189 180 184 187 186 184 180 172 172 171 169 164 162 159 158 155 150 154 154 159 154 161 160 164 164 169 165 169 166 162 158 158 152 143 139 135 128 129 134 147 150 150 161 159 158 164 101 24 37 37 37 38 43 41 41 42 44 41 40 48 46 45 \n",
      "49 53 48 51 51 50 55 48 48 54 51 48 57 48 52 42 17 108 164 168 171 173 170 169 150 161 167 169 176 185 189 176 182 183 188 186 181 174 167 166 167 165 161 160 155 155 154 150 154 154 152 154 155 158 155 164 167 166 165 163 156 151 141 136 133 130 132 132 144 152 154 158 162 160 152 164 104 26 37 33 42 37 40 39 38 46 42 42 45 43 48 47 \n",
      "51 52 49 48 53 47 52 46 48 48 50 52 53 48 53 30 12 114 163 168 171 178 174 171 171 152 159 162 163 178 185 179 184 178 189 184 181 184 174 166 169 167 163 165 156 153 153 151 155 153 150 147 155 164 166 161 166 164 164 160 152 141 131 131 128 134 135 140 151 154 160 162 162 161 157 162 100 27 36 38 39 36 37 46 35 43 39 46 43 48 43 44 \n",
      "48 49 52 47 50 49 51 50 50 48 48 53 47 55 40 24 17 123 160 171 172 180 182 174 179 164 154 154 157 161 175 180 183 185 190 188 184 182 187 170 167 164 165 165 158 160 155 155 154 152 149 154 163 170 167 169 166 164 154 157 145 137 123 129 134 135 145 147 153 162 165 162 163 159 155 166 99 26 38 37 36 38 38 40 41 39 43 43 47 48 45 47 \n",
      "45 49 47 50 50 49 53 45 49 47 52 48 49 48 29 27 11 135 160 166 176 178 183 182 180 176 167 162 154 153 163 174 182 185 190 189 190 185 188 177 164 162 167 167 162 162 159 155 159 155 156 162 169 170 167 171 164 158 148 142 132 130 123 135 138 142 151 153 159 166 172 165 161 158 151 162 94 23 39 38 37 35 39 38 43 41 46 46 48 47 41 52 \n",
      "46 54 50 47 51 48 51 52 49 48 54 50 46 44 26 19 13 150 161 166 177 182 184 186 189 179 177 171 166 152 157 159 174 179 181 186 188 185 186 181 176 172 168 169 166 164 163 163 161 162 163 167 170 165 167 166 163 146 146 128 123 128 133 140 142 148 156 157 168 170 168 168 162 159 149 164 98 24 31 39 34 40 39 38 38 42 42 47 45 46 44 48 \n",
      "49 47 49 47 52 50 48 51 47 49 51 51 39 38 27 21 22 155 161 170 174 181 184 187 190 186 181 178 171 169 156 150 160 174 177 175 182 181 181 182 182 178 169 169 172 168 167 165 164 161 160 164 160 155 158 155 150 138 128 122 130 141 140 140 146 156 165 168 172 173 168 164 160 159 151 159 100 16 33 41 36 37 37 42 38 45 45 42 50 41 48 49 \n",
      "45 52 43 52 48 49 51 52 46 47 51 43 38 37 25 16 14 158 161 171 177 178 184 192 191 190 191 182 181 166 168 163 152 154 160 165 170 176 175 171 170 177 173 170 173 168 170 169 162 159 154 152 148 151 144 145 138 128 125 136 138 142 146 146 149 162 170 172 173 175 169 167 160 156 149 163 99 14 29 42 38 36 35 39 43 43 47 42 48 45 48 45 \n",
      "50 48 50 46 50 47 51 50 48 46 52 35 39 31 25 18 17 139 167 170 175 179 184 189 192 191 195 179 188 172 164 176 169 161 150 152 155 163 165 169 163 167 168 169 167 164 166 161 161 156 153 148 146 142 137 131 128 135 142 143 142 146 146 152 158 165 175 172 174 172 168 168 159 155 149 161 89 16 28 35 41 35 40 41 39 41 46 46 44 45 46 46 \n",
      "45 54 49 46 50 50 47 46 50 47 45 34 39 31 25 19 29 96 173 171 175 175 184 184 195 194 195 188 188 185 173 176 176 171 173 165 153 149 153 162 155 165 163 162 163 160 161 157 156 148 151 145 138 135 132 138 139 146 143 147 143 148 151 148 163 164 180 172 173 174 168 169 159 153 148 157 74 15 28 29 46 37 37 41 39 42 43 50 45 46 47 47 \n",
      "51 51 51 45 52 50 47 48 46 52 34 35 39 31 25 20 47 39 167 170 172 176 179 185 190 195 195 194 190 188 185 180 176 178 179 177 172 166 161 152 145 155 152 151 151 155 152 150 145 143 145 141 140 142 145 145 148 144 144 148 145 152 146 159 170 174 172 171 173 172 170 166 161 150 154 141 57 19 26 31 42 34 39 44 40 41 49 42 44 47 46 46 \n"
     ]
    }
   ],
   "source": [
    "img = read_image('data/orl_faces/s1/1.pgm')\n",
    "for x in range(img.shape[0]):\n",
    "    for y in range(img.shape[1]):\n",
    "        print(img[x,y], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 92)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define another function get_data for generating our data. As we know, for the Siamese network, data should be in the form of pairs (genuine and imposite) with a binary label.\n",
    "\n",
    "First, we read the images (img1, img2) from the same directory and store them in the x_genuine_pair array and assign y_genuine to 1. Next, we read the images (img1, img2) from the different directory and store them in the x_imposite pair and assign y_imposite to 0.\n",
    "\n",
    "Finally, we concatenate both x_genuine_pair, x_imposite to X and y_genuine, y_imposite to Y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_data 함수를 정의할 것이다. 샴 네트워크의 데이터는 이진 라벨의 형태여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2\n",
    "total_sample_size = 10000\n",
    "\n",
    "\n",
    "def get_data(size, total_sample_size):\n",
    "    #read the image\n",
    "    image = read_image('data/orl_faces/s' + str(1) + '/' + str(1) + '.pgm', 'rw+')\n",
    "    #reduce the size\n",
    "    print(image.shape)\n",
    "    image = image[::size, ::size]\n",
    "    print(image.shape)\n",
    "    #get the new size\n",
    "    dim1 = image.shape[0]\n",
    "    dim2 = image.shape[1]\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
    "    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])  # 2 is for pairs\n",
    "    y_genuine = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(40):\n",
    "        for j in range(int(total_sample_size/40)):\n",
    "            ind1 = 0\n",
    "            ind2 = 0\n",
    "            \n",
    "            #read images from same directory (genuine pair)\n",
    "            while ind1 == ind2:\n",
    "                ind1 = np.random.randint(10)\n",
    "                ind2 = np.random.randint(10)\n",
    "            \n",
    "            # read the two images\n",
    "            img1 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+')\n",
    "            img2 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+')\n",
    "            \n",
    "            #reduce the size\n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "            \n",
    "            #store the images to the initialized numpy array\n",
    "            x_geuine_pair[count, 0, 0, :, :] = img1\n",
    "            x_geuine_pair[count, 1, 0, :, :] = img2\n",
    "            \n",
    "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
    "            y_genuine[count] = 1\n",
    "            count += 1\n",
    "\n",
    "    count = 0\n",
    "    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
    "    y_imposite = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(int(total_sample_size/10)):\n",
    "        for j in range(10):\n",
    "            \n",
    "            #read images from different directory (imposite pair)\n",
    "            while True:\n",
    "                ind1 = np.random.randint(40)\n",
    "                ind2 = np.random.randint(40)\n",
    "                if ind1 != ind2:\n",
    "                    break\n",
    "                    \n",
    "            img1 = read_image('data/orl_faces/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
    "            img2 = read_image('data/orl_faces/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
    "\n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "\n",
    "            x_imposite_pair[count, 0, 0, :, :] = img1\n",
    "            x_imposite_pair[count, 1, 0, :, :] = img2\n",
    "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
    "            y_imposite[count] = 0\n",
    "            count += 1\n",
    "            \n",
    "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
    "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
    "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate our data and check our data size. As you can see we have 20,000 data points, out of these 10,000 are genuine pairs and 10,000 are imposite pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터를 생성하고 데이터 크기를 확인한다. 보시다시피 20000 데이터 중 10000개는 genuine 이고, 10000개는 imposite 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 92)\n",
      "(56, 46)\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_data(size, total_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2, 1, 56, 46)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our data for training and testing with 75% training and 25% testing proportions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 훈련용 테스트용으로 나누기 위해 75 / 25 비율로 나눈다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that, we have successfully generated our data, we build our siamese network. First, we define the base network which is basically a convolutional network used for feature extraction. We build two convolutional layers with rectified linear unit (ReLU) activations and max pooling followed by flat layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 성공적으로 데이터를 생성했고 샴 네트워크를 구축했다. 첫째, 기본적으로는 피처 추출에 사용되는 콘볼루션 네트워크인 베이스 네트워크를 정의한다. 우리는 ReLU 활성함수와 Max 풀링에 이어 플랫 레이어를 가진 두 개의 콘볼루션 레이어를 구축한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    \n",
    "    seq = Sequential()\n",
    "    \n",
    "    nb_filter = [6, 12]\n",
    "    kernel_size = 3\n",
    "    \n",
    "    \n",
    "    #convolutional layer 1\n",
    "    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,\n",
    "                          border_mode='valid', dim_ordering='th'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    seq.add(Dropout(.25))\n",
    "    \n",
    "    #convolutional layer 2\n",
    "    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) \n",
    "    seq.add(Dropout(.25))\n",
    "\n",
    "    #flatten \n",
    "    seq.add(Flatten())\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "    seq.add(Dropout(0.1))\n",
    "    seq.add(Dense(50, activation='relu'))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[2:]\n",
    "img_a = Input(shape=input_dim)\n",
    "img_b = Input(shape=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), input_shape=(1, 56, 46..., padding=\"valid\", data_format=\"channels_first\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), padding=\"valid\", data_format=\"channels_first\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n"
     ]
    }
   ],
   "source": [
    "base_network = build_base_network(input_dim)\n",
    "feat_vecs_a = base_network(img_a)\n",
    "feat_vecs_b = base_network(img_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feat_vecs_a and feat_vecs_b are the feature vectors of our image pair. Next, we feed this feature vectors to the energy function to compute the distance between them, we use Euclidean distance as our energy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 feat_vcs_a와 feat_vcs_b는 우리 이미지 쌍의 피처 벡터들이다. 다음으로, 우리는 이 피처 벡터를 에너지 함수에 넣어 그들 사이의 거리를 계산하고, 우리는 우리의 에너지 함수로 유클리드 거리를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we set the epoch length to 13 and we use RMS prop for optimization and define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 13\n",
    "rms = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"la...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model = Model(input=[img_a, img_b], output=distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function as contrastive_loss function and compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=contrastive_loss, optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = x_train[:, 0]\n",
    "img2 = x_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 11250 samples, validate on 3750 samples\n",
      "Epoch 1/13\n",
      " - 40s - loss: 0.1426 - val_loss: 0.0627\n",
      "Epoch 2/13\n",
      " - 35s - loss: 0.0737 - val_loss: 0.0380\n",
      "Epoch 3/13\n",
      " - 35s - loss: 0.0531 - val_loss: 0.0290\n",
      "Epoch 4/13\n",
      " - 35s - loss: 0.0414 - val_loss: 0.0224\n",
      "Epoch 5/13\n",
      " - 34s - loss: 0.0353 - val_loss: 0.0173\n",
      "Epoch 6/13\n",
      " - 34s - loss: 0.0304 - val_loss: 0.0167\n",
      "Epoch 7/13\n",
      " - 34s - loss: 0.0270 - val_loss: 0.0182\n",
      "Epoch 8/13\n",
      " - 35s - loss: 0.0246 - val_loss: 0.0169\n",
      "Epoch 9/13\n",
      " - 34s - loss: 0.0225 - val_loss: 0.0127\n",
      "Epoch 10/13\n",
      " - 34s - loss: 0.0207 - val_loss: 0.0122\n",
      "Epoch 11/13\n",
      " - 34s - loss: 0.0186 - val_loss: 0.0104\n",
      "Epoch 12/13\n",
      " - 40s - loss: 0.0168 - val_loss: 0.0081\n",
      "Epoch 13/13\n",
      " - 38s - loss: 0.0158 - val_loss: 0.0073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fbb372f8a58>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([img_1, img2], y_train, validation_split=.25,\n",
    "          batch_size=128, verbose=2, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make predictions with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터를 통해 예측을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([x_test[:, 0], x_test[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check our model accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 정확성을 체크한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889807162534435"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
